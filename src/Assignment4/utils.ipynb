{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def denormalize_images(images):\n",
    "    \"\"\"Denormalize images from [-1, 1] to [0, 1] range\"\"\"\n",
    "    return (images + 1) / 2\n",
    "\n",
    "def count_model_params(model):\n",
    "    \"\"\" Counting the number of learnable parameters in a nn.Module \"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return num_params\n",
    "\n",
    "def save_model(model, model_name, optimizer, epoch, lambda_kld, stats):\n",
    "    \"\"\" Saving model checkpoint \"\"\"\n",
    "    \n",
    "    # Create the directory path first\n",
    "    save_dir = f\"models/{model_name}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Then create the full file path\n",
    "    savepath = os.path.join(save_dir, f\"checkpoint_KLD_{lambda_kld}_epoch_{epoch}.pth\")\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'stats': stats if stats is not None else None\n",
    "    }, savepath)\n",
    "    return\n",
    "\n",
    "\n",
    "def load_model(model, optimizer, savepath):\n",
    "    \"\"\" Loading pretrained checkpoint \"\"\"\n",
    "    \n",
    "    checkpoint = torch.load(savepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "    stats = checkpoint[\"stats\"]\n",
    "    \n",
    "    return model, optimizer, epoch, stats\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, criterion, lambda_kld, epoch, device, constrained=False):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    vae_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass\n",
    "        if not constrained:\n",
    "            recons, (z, mu, log_var) = model(images)\n",
    "        else:\n",
    "            recons, (z, mu, log_var) = model(images, labels)\n",
    "         \n",
    "        # Calculate Loss\n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var, lambda_kld)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        vae_loss.append(kld.item())\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    \n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, lambda_kld, device, epoch=None, savefig=False, savepath=\"\", writer=None, constrained=False):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    kld_loss = []\n",
    "    \n",
    "    for i, (images, labels) in enumerate(eval_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass \n",
    "        if not constrained:\n",
    "            recons, (z, mu, log_var) = model(images)\n",
    "        else:\n",
    "            recons, (z, mu, log_var) = model(images, labels)\n",
    "                 \n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var, lambda_kld)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        kld_loss.append(kld.item())\n",
    "        \n",
    "        if(i==0 and savefig):\n",
    "            # Denormalize images before saving\n",
    "            recons_denorm = denormalize_images(recons[:36].cpu())\n",
    "            images_denorm = denormalize_images(images[:36].cpu())\n",
    "            \n",
    "            # Save reconstructions\n",
    "            save_image(recons_denorm, os.path.join(savepath, f\"recons_{epoch}.png\"), nrow=6, padding=2, normalize=False)\n",
    "            # Save original images for comparison\n",
    "            save_image(images_denorm, os.path.join(savepath, f\"original_{epoch}.png\"), nrow=6, padding=2, normalize=False)\n",
    "            \n",
    "            if writer is not None:\n",
    "                # Add images to tensorboard\n",
    "                grid_orig = torchvision.utils.make_grid(images_denorm, nrow=6, padding=2, normalize=False)\n",
    "                grid_recon = torchvision.utils.make_grid(recons_denorm, nrow=6, padding=2, normalize=False)\n",
    "                writer.add_image('Original Images', grid_orig, epoch)\n",
    "                writer.add_image('Reconstructed Images', grid_recon, epoch)\n",
    "            \n",
    "    # Total correct predictions and loss\n",
    "    loss = np.mean(loss_list)\n",
    "    recons_loss = np.mean(recons_loss)\n",
    "    kld_loss = np.mean(kld_loss)\n",
    "    return loss, recons_loss, kld_loss\n",
    "\n",
    "\n",
    "def train_model(model, model_name, optimizer, scheduler, criterion, lambda_kld, train_loader, valid_loader,\n",
    "                num_epochs, savepath, writer, save_frequency=5, vis_frequency=2, constrained=False):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    val_loss_recons =  []\n",
    "    val_loss_kld =  []\n",
    "    loss_iters = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        log_epoch = (epoch % vis_frequency == 0 or epoch == num_epochs - 1)\n",
    "        loss, recons_loss, kld_loss = eval_model(\n",
    "                model=model, eval_loader=valid_loader, criterion=criterion, lambda_kld=lambda_kld,\n",
    "                device=device, epoch=epoch, savefig=log_epoch, savepath=savepath,\n",
    "                writer=writer, constrained=constrained\n",
    "            )\n",
    "        val_loss.append(loss)\n",
    "        val_loss_recons.append(recons_loss)\n",
    "        val_loss_kld.append(kld_loss)\n",
    "\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/All_Valid_Loss', {\"recons\": recons_loss.item(), \"kld\": kld_loss.item()}, global_step=epoch)\n",
    "        \n",
    "        for param_group in optimizer.param_groups:\n",
    "            lr = param_group['lr']\n",
    "            writer.add_scalar('Learning Rate', lr, global_step=epoch)\n",
    "        \n",
    "        writer.add_scalar(f'Lambda/KLD', lambda_kld, global_step=epoch)\n",
    "\n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, lambda_kld=lambda_kld, epoch=epoch, device=device, constrained=constrained\n",
    "            )\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/Comb', {\"train\": mean_loss.item(), \"valid\": loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # PLATEAU SCHEDULER\n",
    "        scheduler.step(val_loss[-1])\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % save_frequency == 0):\n",
    "            stats = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"loss_iters\": loss_iters\n",
    "            }\n",
    "            # save_model(model=model, model_name=model_name, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        \n",
    "        if(log_epoch):\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"       Valid loss recons: {round(val_loss_recons[-1], 5)}\")\n",
    "            print(f\"       Valid loss KL-D:   {round(val_loss_kld[-1], 5)}\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld\n",
    "\n",
    "def img_vs_recons(model, test_loader, device):\n",
    "\n",
    "    imgs, _ = next(iter(test_loader)) \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recons, _ = model(imgs.to(device))\n",
    "        fig, ax = plt.subplots(2, 8)\n",
    "        fig.set_size_inches(18, 5)\n",
    "        for i in range(8):\n",
    "            # Denormalize images from [-1, 1] to [0, 1] range\n",
    "            orig_img = denormalize_images(imgs[i].cpu())\n",
    "            recon_img = denormalize_images(recons[i].cpu())\n",
    "            \n",
    "            # Convert from (C,H,W) to (H,W,C) format for imshow\n",
    "            orig_img = orig_img.permute(1, 2, 0)\n",
    "            recon_img = recon_img.permute(1, 2, 0)\n",
    "            \n",
    "            ax[0, i].imshow(orig_img)\n",
    "            ax[0, i].axis(\"off\")\n",
    "            ax[1, i].imshow(recon_img)\n",
    "            ax[1, i].axis(\"off\")\n",
    "\n",
    "        ax[0, 3].set_title(\"Original Image\")\n",
    "        ax[1, 3].set_title(\"Reconstruction\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_recons(recons):\n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        # recon_img = denormalize_images(recons[i].cpu())\n",
    "        recon_img = recons[i].cpu()\n",
    "        recon_img = recon_img.permute(1, 2, 0)\n",
    "        plt.imshow(recon_img)  \n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "COLORS = ['r', 'b', 'g', 'y', 'purple', 'orange', 'k', 'brown', 'grey',\n",
    "          'c', \"gold\", \"fuchsia\", \"lime\", \"darkred\", \"tomato\", \"navy\"]\n",
    "\n",
    "def display_projections(points, labels, ax=None, legend=None):\n",
    "    \"\"\" Displaying low-dimensional data projections \"\"\"\n",
    "    \n",
    "    legend = [f\"Class {l}\" for l in np.unique(labels)] if legend is None else legend\n",
    "    if(ax is None):\n",
    "        _, ax = plt.subplots(1,1,figsize=(12,6))\n",
    "    \n",
    "    for i,l in enumerate(np.unique(labels)):\n",
    "        idx = np.where(l==labels)\n",
    "\n",
    "        ax.scatter(points[idx, 0], points[idx, 1], label=legend[int(l)], c=COLORS[i])\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def plot_reconstructed(model, xrange=(-3, 3), yrange=(-2, 2), N=12):\n",
    "#     \"\"\"\n",
    "#     Sampling equispaced points from the latent space given the xrange and yrange, \n",
    "#     decoding latents and visualizing distribution of the space\n",
    "#     \"\"\"\n",
    "#     # Project points to decoder input dimension (same as in forward pass)\n",
    "#     SIZE = 64  # Image size\n",
    "#     grid = np.empty((N*SIZE, N*SIZE, 3))  # 3 channels for RGB\n",
    "    \n",
    "#     for i, y in enumerate(np.linspace(*yrange, N)):\n",
    "#         for j, x in enumerate(np.linspace(*xrange, N)):\n",
    "#             # mean\n",
    "#             mu = torch.zeros(model.latent_dim, device=device)\n",
    "#             mu[0] = x\n",
    "#             mu[1] = y\n",
    "            \n",
    "#             # standard deviation\n",
    "#             sigma = 1 \n",
    "#             z = torch.normal(mean=mu, std=sigma)\n",
    "            \n",
    "#             # Passing through the decoder \n",
    "#             z = model.decoder_input(z)\n",
    "#             z = z.view(-1, 256, 4, 4)  # Reshape to match decoder input\n",
    "            \n",
    "#             # Getting recons\n",
    "#             x_hat = model.decoder(z)\n",
    "            \n",
    "#             # To visualize\n",
    "#             x_hat = x_hat.squeeze(0).cpu()  # Remove batch dimension\n",
    "#             x_hat = x_hat.permute(1, 2, 0)  # (C,H,W) to (H,W,C)\n",
    "#             x_hat = x_hat.numpy()\n",
    "            \n",
    "#             # Enhance contrast\n",
    "#             x_hat = np.clip(x_hat, 0, 1)  # Multiply by 1.2 to increase contrast, then clip to valid range\n",
    "            \n",
    "#             grid[(N-1-i)*SIZE:(N-i)*SIZE, j*SIZE:(j+1)*SIZE] = x_hat\n",
    "           \n",
    "#     plt.figure(figsize=(12,20))\n",
    "#     plt.imshow(grid, extent=[*yrange, *xrange])\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_reconstructed(model, xrange=(-3, 3), yrange=(-2, 2), N=12):\n",
    "    \"\"\"\n",
    "    Sampling equispaced points from the latent space given the xrange and yrange, \n",
    "    decoding latents and visualizing distribution of the space\n",
    "    \"\"\"\n",
    "    # Project points to decoder input dimension (same as in forward pass)\n",
    "    SIZE = 64  # Image size\n",
    "    grid = np.empty((N*SIZE, N*SIZE, 3))  # 3 channels for RGB\n",
    "    \n",
    "    for i, y in enumerate(np.linspace(*yrange, N)):\n",
    "        for j, x in enumerate(np.linspace(*xrange, N)):\n",
    "            # mean\n",
    "            mu = torch.zeros(model.latent_dim, device=device)\n",
    "            mu[0] = x\n",
    "            mu[1] = y\n",
    "            \n",
    "            # standard deviation\n",
    "            sigma = 1 \n",
    "            std = torch.full_like(mu, sigma)\n",
    "            z = torch.normal(mean=mu, std=std)\n",
    "            \n",
    "            # Passing through the decoder \n",
    "            z = model.decoder_input(z)\n",
    "            enc_output_shape, _ = compute_encoder_output_size(64, (3, 64, 64), model)\n",
    "            z = z.view(-1, *enc_output_shape)  # Reshape to match decoder input\n",
    "            \n",
    "            # Getting recons\n",
    "            x_hat = model.decoder(z)\n",
    "            \n",
    "            # To visualize\n",
    "            x_hat = x_hat.squeeze(0).cpu()  # Remove batch dimension\n",
    "            x_hat = x_hat.permute(1, 2, 0)  # (C,H,W) to (H,W,C)\n",
    "            recon_img = x_hat.numpy()\n",
    "            \n",
    "            # Prepare the image for visualization\n",
    "            recon_img = denormalize_images(recon_img[i])\n",
    "            # recon_img = recon_img.permute(1, 2, 0)\n",
    "            \n",
    "            grid[(N-1-i)*SIZE:(N-i)*SIZE, j*SIZE:(j+1)*SIZE] = recon_img\n",
    "           \n",
    "    plt.figure(figsize=(12,20))\n",
    "    plt.imshow(grid, extent=[*yrange, *xrange])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def vae_loss_function(recons, target, mu, log_var, lambda_kld=0.0):\n",
    "    \"\"\"\n",
    "    Combined loss function for joint optimization of \n",
    "    reconstruction and ELBO\n",
    "    \"\"\"\n",
    "    recons_loss = F.mse_loss(recons, target)\n",
    "    kld = (-0.5 * (1 + log_var - mu**2 - log_var.exp()).sum(dim=1)).mean(dim=0)  # closed-form solution of KLD in Gaussian\n",
    "    loss = recons_loss + lambda_kld * kld\n",
    "\n",
    "    return loss, (recons_loss, kld)\n",
    "\n",
    "def makedires(configs):\n",
    "    model_name = configs[\"model_name\"]+configs[\"exp\"]+f\"_KLD_{configs['lambda_kld']}\"\n",
    "    savepath = f\"imgs/{model_name}\"\n",
    "    if os.path.exists(savepath):\n",
    "        shutil.rmtree(savepath)\n",
    "    os.makedirs(savepath,exist_ok=True)\n",
    "\n",
    "    TBOARD_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", model_name)\n",
    "    if os.path.exists(TBOARD_LOGS):\n",
    "        shutil.rmtree(TBOARD_LOGS)\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "    writer = SummaryWriter(TBOARD_LOGS)\n",
    "    return savepath, writer\n",
    "\n",
    "def save_config(configs):\n",
    "    model_name = configs[\"model_name\"]+configs[\"exp\"]+f\"_KLD_{configs['lambda_kld']}\"\n",
    "    configs_dir = f\"./configs/{model_name}/\"\n",
    "    if not os.path.exists(configs_dir):\n",
    "        os.makedirs(configs_dir,exist_ok=True)\n",
    "    configs_path = configs_dir + \"/config.yaml\"\n",
    "\n",
    "    with open(configs_path, 'w') as f:\n",
    "        yaml.dump(configs, f)\n",
    "\n",
    "\n",
    "def compute_encoder_output_size(batch_size, input_shape, model):\n",
    "    \"\"\"\n",
    "    Compute the un/flattened output size of the encoder given an input shape.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "\n",
    "        enc_input = torch.zeros(batch_size, *input_shape).to(device)  # (BS, 3, 64, 64)\n",
    "        \n",
    "        # Get encoder output\n",
    "        output = model.encoder(enc_input)\n",
    "        \n",
    "        enc_out_shape = output.view(batch_size,-1,4,4).shape[1:]\n",
    "        # Get flattened size\n",
    "        flattened_size = output.view(1, -1).shape[1]\n",
    "        \n",
    "    return enc_out_shape, flattened_size\n",
    "\n",
    "def inference(configs, model):\n",
    "    model_name = configs[\"model_name\"]+configs[\"exp\"]+f\"_KLD_{configs['lambda_kld']}\"\n",
    "    BS = configs[\"batch_size\"]\n",
    "    if not os.path.exists(f\"imgs/inference/{model_name}\"):\n",
    "        os.makedirs(f\"imgs/inference/{model_name}\")\n",
    "\n",
    "    latent_dim = configs[\"latent_dim\"]\n",
    "\n",
    "    enc_output_shape, _ = compute_encoder_output_size(BS, (3, 64, 64), model)\n",
    "    # print(enc_output_shape)\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            z = torch.randn(BS, latent_dim).to(device)\n",
    "\n",
    "            z = model.decoder_input(z)\n",
    "            z = z.view(-1, *enc_output_shape)\n",
    "\n",
    "            recons = model.decoder(z)\n",
    "            recons = recons.view(BS, 3, 64, 64)\n",
    "            save_image(recons, f\"imgs/inference/{model_name}/inference_{i}.png\")\n",
    "            return recons\n",
    "\n",
    "def plot_recons(recons):\n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        recon_img = denormalize_images(recons[i].cpu())\n",
    "        # recon_img = recons[i].cpu()\n",
    "        recon_img = recon_img.permute(1, 2, 0)\n",
    "        plt.imshow(recon_img)  \n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def vis_latent(test_loader, model, test_dataset):\n",
    "    imgs_flat, latents, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            _, (z, _, _) = model(imgs)\n",
    "            imgs_flat.append(imgs.cpu().view(imgs.shape[0],-1))\n",
    "            latents.append(z.cpu())\n",
    "            labels.append(lbls)\n",
    "            \n",
    "    imgs_flat = np.concatenate(imgs_flat)    \n",
    "    latents = np.concatenate(latents)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    latents_reshaped = latents.reshape(latents.shape[0], -1)\n",
    "\n",
    "    pca_imgs = PCA(n_components=2).fit_transform(imgs_flat)\n",
    "    pca_latents = PCA(n_components=2).fit_transform(latents_reshaped)\n",
    "\n",
    "    N = 2000\n",
    "    fig,ax = plt.subplots(1,2,figsize=(26,8))\n",
    "    display_projections(pca_imgs[:N], labels[:N], ax=ax[0], legend=test_dataset.classes)\n",
    "    ax[0].set_title(\"PCA Proj. of Images\")\n",
    "    display_projections(pca_latents[:N], labels[:N], ax=ax[1], legend=test_dataset.classes)\n",
    "    ax[1].set_title(\"Encoded Representations\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_stats(dataset, channels = 3):\n",
    "    \"\"\"Computing mean and std of dataset\"\"\"\n",
    "    mean = torch.zeros(channels)\n",
    "    std = torch.zeros(channels)\n",
    "    num_samples = 0\n",
    "\n",
    "    for img, _ in tqdm(dataset):  # img shape: [3, H, W]\n",
    "        mean += img.mean(dim=(1, 2))  # Per-channel mean\n",
    "        std += img.std(dim=(1, 2))    # Per-channel std\n",
    "        num_samples += 1\n",
    "\n",
    "    mean /= num_samples\n",
    "    std /= num_samples\n",
    "    return mean, std\n",
    "\n",
    "def get_activation(act_name):\n",
    "    \"\"\" Gettign activation given name \"\"\"\n",
    "    assert act_name in [\"ReLU\", \"Sigmoid\", \"Tanh\"]\n",
    "    activation = getattr(nn, act_name)\n",
    "    return activation()\n",
    "\n",
    "def get_dropout(drop_p):\n",
    "    \"\"\" Getting a dropout layer \"\"\"\n",
    "    if(drop_p):\n",
    "        drop = nn.Dropout(p=drop_p)\n",
    "    else:\n",
    "        drop = nn.Identity()\n",
    "    return drop"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
