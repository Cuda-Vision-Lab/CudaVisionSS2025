{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    " - a) We will be working with the **AFHQ (Animal Faces-HQ)** dataset: https://github.com/clovaai/stargan-v2\n",
    "    - Approx 15.000 images of animal faces (cat, dog, tiger, ...)\n",
    "    - Downsample the images to 64x64\n",
    " - b) Write **Convolutional** Variational Autoencoder (ConvVAE)\n",
    "      - Use Conv. layers for encoder and Transposed-Conv. layers for decoder.\n",
    "      - You are only allowed to use linear layers for estimating the mean and standard deviation. Everything else should be convolutional.\n",
    " - c) Investigate the importance of the KL-divergence weight. For this purpose, train multiple models (at least 4) using different weighting values and investigate how this value affects the generation performance.\n",
    " - d) Generate new images by sampling latent vectors, investigate latent space and visualize some interpolations.\n",
    " - e) Compare the models from b) and c)\n",
    "     - Qualitative comparison. Which images look better?\n",
    "     - Quantitative comparison between models using the FrÃ©chet Inception Distance: https://arxiv.org/abs/1706.08500\n",
    "     - Log generated images and losses into the Tensorboard/W&B\n",
    "     \n",
    "     \n",
    "**Extra Point:**\n",
    " - Extend your ConvVAE for Image generation conditioned on a given class. The AFHQ dataset has 3 classes: 'cat', 'dog', and 'wildlife'\n",
    " - Train your Conditional-ConvVAE\n",
    " - Show that you can generate images conditioned on a label\n",
    " - Tutorial: https://ijdykeman.github.io/ml/2016/12/21/cvae.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 15:29:16.633552: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-03 15:29:16.643916: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748957356.656104   17896 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748957356.659720   17896 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748957356.669089   17896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748957356.669119   17896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748957356.669121   17896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748957356.669122   17896 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-03 15:29:16.672332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from utils import *\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"data_dir\": \"data/AFHQ\",\n",
    "        \"img_size\": 64,\n",
    "        \"img_channels\": 3,\n",
    "        \"batch_size\": 256,\n",
    "        \"num_workers\": 8,\n",
    "        \"savepath\" : \"imgs/vanilla_vae\",\n",
    "        \"num_epochs\": 15\n",
    "        }\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing mean, std of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computed:\\nMean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\\nStd  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=config[\"data_dir\"]+\"/train\",\n",
    "    transform=transforms.ToTensor()  # Converts images to [C, H, W] with values in [0, 1]\n",
    ")\n",
    "\"\"\"mean, std = compute_stats(dataset)\n",
    "print(f\"Mean (R, G, B): {mean.tolist()}\")\n",
    "print(f\"Std  (R, G, B): {std.tolist()}\")\n",
    "\"\"\"\n",
    "\n",
    "mean = torch.tensor([0.5018709301948547, 0.4601306915283203, 0.3988320827484131])\n",
    "std = torch.tensor([0.22490206360816956, 0.2184346467256546, 0.21783076226711273])\n",
    "\"\"\"computed:\n",
    "Mean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\n",
    "Std  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14336 images\n",
      "Valdiation set size: 1467 images\n",
      "['cat', 'dog', 'wild']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"train\"), data_transforms[\"train\"])\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"test\"), data_transforms[\"val\"])\n",
    "\n",
    "N_train = len(train_dataset)\n",
    "N_valid = len(valid_dataset)\n",
    "print(f\"Training set size: {N_train} images\")\n",
    "print(f\"Valdiation set size: {N_valid} images\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "\n",
    "#for i, (images, _) in enumerate(valid_loader):\n",
    "#    print(images.shape)\n",
    "#    break\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(data, titles=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    data = data.numpy().transpose((0, 2, 3, 1))\n",
    "    data = std * data + mean  # IMPORTANT! If you normalize imgs in the DataLoader, undo the norm. for visualization\n",
    "    data = np.clip(data, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.axis(\"off\")\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "            \n",
    "# Get a batch of training data and displaying it\n",
    "#inputs, classes = next(iter(train_loader))\n",
    "#inputs, classes = next(iter(valid_loader))\n",
    "#titles = [class_names[x] for x in classes]\n",
    "#inputs.shape\n",
    "#show_grid(inputs, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, in_size=(3, 32, 32), \n",
    "                 channel_sizes=[3,16,32,64,128],\n",
    "                 latent_dim=64,\n",
    "                 act_final=\"Sigmoid\",\n",
    "                 act_hidden=\"ReLU\", \n",
    "                 kernel_size=3, \n",
    "                 padding=1,\n",
    "                 stride=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.activation_hidden = get_activation(act_hidden)\n",
    "        self.activation_final= get_activation(act_final)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder,self.latent_input_size = self._make_encoder()\n",
    "        self.decoder = self._make_decoder()\n",
    "        self.fc_mu = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        self.fc_sigma = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        return\n",
    "        \n",
    "    def _make_encoder(self):\n",
    "        \"\"\" Defining encoder \"\"\"\n",
    "        #layers = [nn.Flatten()]\n",
    "        layers = []\n",
    "        for i in range(len(self.channel_sizes)-1):\n",
    "            layers.append( nn.Conv2d(in_channels=self.channel_sizes[i], \n",
    "                                     out_channels=self.channel_sizes[i+1], \n",
    "                                     kernel_size=self.kernel_size, \n",
    "                                     padding=self.padding,\n",
    "                                     stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[i+1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "            if i ==0:\n",
    "                img_size = compute_image_size(list(self.in_size[1:3]), np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "            else:\n",
    "                img_size = compute_image_size(img_size, np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "        \n",
    "        layers.append(nn.Flatten())\n",
    "        encoder = nn.Sequential(*layers)\n",
    "        latent_input_size=int(np.prod(img_size)*self.channel_sizes[-1])\n",
    "        return encoder, latent_input_size\n",
    "    \n",
    "    def _make_decoder(self):\n",
    "        \"\"\" Defining decoder \"\"\"\n",
    "        layers = [nn.Linear(in_features=self.latent_dim,out_features=self.latent_input_size),self.activation_hidden]\n",
    "        \n",
    "        for i in range(1, len(self.channel_sizes)):\n",
    "            layers.append( nn.ConvTranspose2d(in_channels=self.channel_sizes[-i], \n",
    "                                              out_channels=self.channel_sizes[-i-1], \n",
    "                                              kernel_size=self.kernel_size, \n",
    "                                              padding=self.padding,\n",
    "                                              stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[-i-1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "\n",
    "        layers = layers[:-1] + [self.activation_final]\n",
    "        decoder = nn.Sequential(*layers)\n",
    "        return decoder\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\" Reparametrization trick\"\"\"\n",
    "        std = torch.exp(0.5*log_var)  # we can also predict the std directly, but this works best\n",
    "        eps = torch.randn_like(std)  # random sampling happens here\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        print(f\"x Input shape: {x.shape}\")\n",
    "        x_enc = self.encoder(x)\n",
    "        print(f\"x_enc Input shape: {x_enc.shape}\")\n",
    "        \n",
    "        mu = self.fc_mu(x_enc)\n",
    "        log_var = self.fc_sigma(x_enc)\n",
    "        print(f\"mu shape: {mu.shape}, log_var shape: {log_var.shape}\")\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        print(f\"z shape: {z.shape}\")\n",
    "        x_hat_flat = self.decoder(z)\n",
    "        print(f\"x_hat_flat shape: {x_hat_flat.shape}\")\n",
    "        x_hat = x_hat_flat.view(-1, *self.in_size)\n",
    "        print(f\"x_hat shape: {x_hat.shape}\")\n",
    "        return x_hat, (z, mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_loss_function(recons, target, mu, log_var, lambda_kld=1e-3):\n",
    "    recons_loss = F.mse_loss(recons, target)\n",
    "    kld = (-0.5 * (1 + log_var - mu**2 - log_var.exp()).sum(dim=1)).mean(dim=0)  # closed-form solution of KLD in Gaussian\n",
    "    loss = recons_loss + lambda_kld * kld\n",
    "    return loss, (recons_loss, kld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    vae_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, _) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass\n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "         \n",
    "        # Calculate Loss\n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        vae_loss.append(kld.item())\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    \n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device, epoch=None, savefig=False, savepath=\"\", writer=None):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    kld_loss = []\n",
    "    \n",
    "    for i, (images, _) in enumerate(eval_loader):\n",
    "        images = images.to(device)\n",
    "        print(f\"****{images.shape=}\")\n",
    "        # Forward pass \n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "                 \n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        kld_loss.append(kld.item())\n",
    "        \n",
    "        if(i==0 and savefig):\n",
    "            save_image( recons[:64].cpu(), os.path.join(savepath, f\"recons{epoch}.png\") )\n",
    "            if writer is not None:\n",
    "                grid = torchvision.utils.make_grid(images[:64].cpu())\n",
    "                writer.add_image('images', grid, epoch)\n",
    "                grid = torchvision.utils.make_grid(recons[:64].cpu())\n",
    "                writer.add_image('output_images', grid, epoch)\n",
    "            \n",
    "    # Total correct predictions and loss\n",
    "    loss = np.mean(loss_list)\n",
    "    recons_loss = np.mean(recons_loss)\n",
    "    kld_loss = np.mean(kld_loss)\n",
    "    return loss, recons_loss, kld_loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader,\n",
    "                num_epochs, savepath, writer, save_frequency=5, vis_frequency=2):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    val_loss_recons =  []\n",
    "    val_loss_kld =  []\n",
    "    loss_iters = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        log_epoch = (epoch % vis_frequency == 0 or epoch == num_epochs - 1)\n",
    "        loss, recons_loss, kld_loss = eval_model(\n",
    "                model=model, eval_loader=valid_loader, criterion=criterion,\n",
    "                device=device, epoch=epoch, savefig=log_epoch, savepath=savepath,\n",
    "                writer=writer\n",
    "            )\n",
    "        val_loss.append(loss)\n",
    "        val_loss_recons.append(recons_loss)\n",
    "        val_loss_kld.append(kld_loss)\n",
    "\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/All_Valid_Loss', {\"recons\": recons_loss.item(), \"kld\": kld_loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/Comb', {\"train\": mean_loss.item(), \"valid\": loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # PLATEAU SCHEDULER\n",
    "        scheduler.step(val_loss[-1])\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % save_frequency == 0):\n",
    "            stats = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"loss_iters\": loss_iters\n",
    "            }\n",
    "            save_model(model=model, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        \n",
    "        if(log_epoch):\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"       Valid loss recons: {round(val_loss_recons[-1], 5)}\")\n",
    "            print(f\"       Valid loss KL-D:   {round(val_loss_kld[-1], 5)}\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model param count=590857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aidin/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvolutionalVAE(\n",
       "  (activation_hidden): ReLU()\n",
       "  (activation_final): Sigmoid()\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Sigmoid()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2048, out_features=64, bias=True)\n",
       "  (fc_sigma): Linear(in_features=2048, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVAE(in_size=(config[\"img_channels\"],config[\"img_size\"],config[\"img_size\"]), \n",
    "                         channel_sizes=[3,16,32,64,128], \n",
    "                         act_hidden=\"ReLU\",\n",
    "                         latent_dim=64,\n",
    "                         stride=2,\n",
    "                         padding=1,\n",
    "                         kernel_size=3).to(device)\n",
    "#print(model)\n",
    "print(f\"model param count={count_model_params(model)}\")\n",
    "writer=getTensorboardWriter(params=[os.getcwd(),\"tboard_logs\", \"cvae\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "# Decay LR by a factor of 10 after 5 epochs with no improvement\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1, verbose=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "x Input shape: torch.Size([256, 3, 64, 64])\n",
      "x_enc Input shape: torch.Size([256, 2048])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msavepath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, savepath, writer, save_frequency, vis_frequency)\u001b[0m\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# important for dropout and batch norms\u001b[39;00m\n\u001b[1;32m     84\u001b[0m log_epoch \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m%\u001b[39m vis_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m loss, recons_loss, kld_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavefig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     91\u001b[0m val_loss_recons\u001b[38;5;241m.\u001b[39mappend(recons_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, eval_loader, criterion, device, epoch, savefig, savepath, writer)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m****\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Forward pass \u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m recons, (z, mu, log_var) \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     50\u001b[0m loss, (mse, kld) \u001b[38;5;241m=\u001b[39m criterion(recons, images, mu, log_var)\n\u001b[1;32m     51\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=vae_loss_function,\n",
    "        train_loader=train_loader, valid_loader=valid_loader, num_epochs=config[\"num_epochs\"], savepath=config[\"savepath\"],\n",
    "        writer=writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_image_size(config[\"img_size\"], model.kernel_size, model.padding, model.stride)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
