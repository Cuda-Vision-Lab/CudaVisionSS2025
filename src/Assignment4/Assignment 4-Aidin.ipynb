{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    " - a) We will be working with the **AFHQ (Animal Faces-HQ)** dataset: https://github.com/clovaai/stargan-v2\n",
    "    - Approx 15.000 images of animal faces (cat, dog, tiger, ...)\n",
    "    - Downsample the images to 64x64\n",
    " - b) Write **Convolutional** Variational Autoencoder (ConvVAE)\n",
    "      - Use Conv. layers for encoder and Transposed-Conv. layers for decoder.\n",
    "      - You are only allowed to use linear layers for estimating the mean and standard deviation. Everything else should be convolutional.\n",
    " - c) Investigate the importance of the KL-divergence weight. For this purpose, train multiple models (at least 4) using different weighting values and investigate how this value affects the generation performance.\n",
    " - d) Generate new images by sampling latent vectors, investigate latent space and visualize some interpolations.\n",
    " - e) Compare the models from b) and c)\n",
    "     - Qualitative comparison. Which images look better?\n",
    "     - Quantitative comparison between models using the Fr√©chet Inception Distance: https://arxiv.org/abs/1706.08500\n",
    "     - Log generated images and losses into the Tensorboard/W&B\n",
    "     \n",
    "     \n",
    "**Extra Point:**\n",
    " - Extend your ConvVAE for Image generation conditioned on a given class. The AFHQ dataset has 3 classes: 'cat', 'dog', and 'wildlife'\n",
    " - Train your Conditional-ConvVAE\n",
    " - Show that you can generate images conditioned on a label\n",
    " - Tutorial: https://ijdykeman.github.io/ml/2016/12/21/cvae.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 12:16:01.597329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 12:16:02.120693: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from utils import *\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"data_dir\": \"data/AFHQ\",\n",
    "        \"img_size\": 64,\n",
    "        \"img_channels\": 3,\n",
    "        \"batch_size\": 256,\n",
    "        \"num_workers\": 8,\n",
    "        \"savepath\" : \"imgs/vanilla_vae\",\n",
    "        \"num_epochs\": 15\n",
    "        }\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing mean, std of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computed:\\nMean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\\nStd  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=config[\"data_dir\"]+\"/train\",\n",
    "    transform=transforms.ToTensor()  # Converts images to [C, H, W] with values in [0, 1]\n",
    ")\n",
    "\"\"\"mean, std = compute_stats(dataset)\n",
    "print(f\"Mean (R, G, B): {mean.tolist()}\")\n",
    "print(f\"Std  (R, G, B): {std.tolist()}\")\n",
    "\"\"\"\n",
    "\n",
    "mean = torch.tensor([0.5018709301948547, 0.4601306915283203, 0.3988320827484131])\n",
    "std = torch.tensor([0.22490206360816956, 0.2184346467256546, 0.21783076226711273])\n",
    "\"\"\"computed:\n",
    "Mean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\n",
    "Std  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14336 images\n",
      "Valdiation set size: 1467 images\n",
      "['cat', 'dog', 'wild']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"train\"), data_transforms[\"train\"])\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"test\"), data_transforms[\"val\"])\n",
    "\n",
    "N_train = len(train_dataset)\n",
    "N_valid = len(valid_dataset)\n",
    "print(f\"Training set size: {N_train} images\")\n",
    "print(f\"Valdiation set size: {N_valid} images\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "\n",
    "#for i, (images, _) in enumerate(valid_loader):\n",
    "#    print(images.shape)\n",
    "#    break\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(data, titles=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    data = data.numpy().transpose((0, 2, 3, 1))\n",
    "    data = std * data + mean  # IMPORTANT! If you normalize imgs in the DataLoader, undo the norm. for visualization\n",
    "    data = np.clip(data, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.axis(\"off\")\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "            \n",
    "# Get a batch of training data and displaying it\n",
    "#inputs, classes = next(iter(train_loader))\n",
    "#inputs, classes = next(iter(valid_loader))\n",
    "#titles = [class_names[x] for x in classes]\n",
    "#inputs.shape\n",
    "#show_grid(inputs, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, in_size=(3, 32, 32), \n",
    "                 channel_sizes=[3,16,32,64,128],\n",
    "                 latent_dim=64,\n",
    "                 act_final=\"Sigmoid\",\n",
    "                 act_hidden=\"ReLU\", \n",
    "                 kernel_size=3, \n",
    "                 padding=1,\n",
    "                 stride=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.activation_hidden = get_activation(act_hidden)\n",
    "        self.activation_final= get_activation(act_final)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder,self.latent_input_size = self._make_encoder()\n",
    "        self.decoder = self._make_decoder()\n",
    "        self.fc_mu = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        self.fc_sigma = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        self.latent_to_decoder=nn.Sequential (nn.Linear(latent_dim,self.latent_input_size))\n",
    "        return\n",
    "        \n",
    "    def _make_encoder(self):\n",
    "        \"\"\" Defining encoder \"\"\"\n",
    "        #layers = [nn.Flatten()]\n",
    "        layers = []\n",
    "        for i in range(len(self.channel_sizes)-1):\n",
    "            layers.append( nn.Conv2d(in_channels=self.channel_sizes[i], \n",
    "                                     out_channels=self.channel_sizes[i+1], \n",
    "                                     kernel_size=self.kernel_size, \n",
    "                                     padding=self.padding,\n",
    "                                     stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[i+1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "            if i ==0:\n",
    "                img_size = compute_image_size(list(self.in_size[1:3]), np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "            else:\n",
    "                img_size = compute_image_size(img_size, np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "        layers.append(nn.Flatten())\n",
    "        encoder = nn.Sequential(*layers)\n",
    "        latent_input_size=int(np.prod(img_size)*self.channel_sizes[-1])\n",
    "        return encoder, latent_input_size\n",
    "    \n",
    "    def _make_decoder(self):\n",
    "        \"\"\" Defining decoder \"\"\"\n",
    "        layers = []#[nn.Linear(in_features=self.latent_dim,out_features=self.latent_input_size),self.activation_hidden]\n",
    "        \n",
    "        for i in range(1, len(self.channel_sizes)):\n",
    "            layers.append( nn.ConvTranspose2d(in_channels=self.channel_sizes[-i], \n",
    "                                              out_channels=self.channel_sizes[-i-1], \n",
    "                                              output_padding=1,\n",
    "                                              kernel_size=self.kernel_size, \n",
    "                                              padding=self.padding,\n",
    "                                              stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[-i-1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "\n",
    "        layers = layers[:-2] + [self.activation_final]\n",
    "        decoder = nn.Sequential(*layers)\n",
    "        return decoder\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\" Reparametrization trick\"\"\"\n",
    "        std = torch.exp(0.5*log_var)  # we can also predict the std directly, but this works best\n",
    "        eps = torch.randn_like(std)  # random sampling happens here\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    def get_image_dimensions_for_decoder(self,z_to_decoder):\n",
    "        squared_img_size=z_to_decoder.shape[-1]/ self.channel_sizes[-1]\n",
    "        image_size= int(np.sqrt(squared_img_size))\n",
    "        z_to_decoder = z_to_decoder.view(-1, self.channel_sizes[-1], image_size, image_size)\n",
    "        return z_to_decoder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        x_enc = self.encoder(x)\n",
    "        mu = self.fc_mu(x_enc)\n",
    "        log_var = self.fc_sigma(x_enc)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        z_to_decoder=self.latent_to_decoder(z)\n",
    "        z_to_decoder=self.get_image_dimensions_for_decoder(z_to_decoder)\n",
    "        x_hat_flat = self.decoder(z_to_decoder)\n",
    "        x_hat = x_hat_flat.view(-1, *self.in_size)\n",
    "        return x_hat, (z, mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_loss_function(recons, target, mu, log_var, lambda_kld=1e-3):\n",
    "    recons_loss = F.mse_loss(recons, target)\n",
    "    kld = (-0.5 * (1 + log_var - mu**2 - log_var.exp()).sum(dim=1)).mean(dim=0)  # closed-form solution of KLD in Gaussian\n",
    "    loss = recons_loss + lambda_kld * kld\n",
    "    return loss, (recons_loss, kld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    vae_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, _) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass\n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "         \n",
    "        # Calculate Loss\n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        vae_loss.append(kld.item())\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    \n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device, epoch=None, savefig=False, savepath=\"\", writer=None):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    kld_loss = []\n",
    "    trace=0\n",
    "    for i, (images, _) in enumerate(eval_loader):\n",
    "        images = images.to(device)\n",
    "        print(f\"****{images.shape=}\")\n",
    "        # Forward pass \n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        kld_loss.append(kld.item())\n",
    "        if(i==0 and savefig):\n",
    "            save_image( recons[:64].cpu(), os.path.join(savepath, f\"recons{epoch}.png\") )\n",
    "            if writer is not None:\n",
    "                grid = torchvision.utils.make_grid(images[:64].cpu())\n",
    "                writer.add_image('images', grid, epoch)\n",
    "                grid = torchvision.utils.make_grid(recons[:64].cpu())\n",
    "                writer.add_image('output_images', grid, epoch)\n",
    "    # Total correct predictions and loss\n",
    "    loss = np.mean(loss_list)\n",
    "    recons_loss = np.mean(recons_loss)\n",
    "    kld_loss = np.mean(kld_loss)\n",
    "    return loss, recons_loss, kld_loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader,\n",
    "                num_epochs, savepath, writer, save_frequency=5, vis_frequency=2):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    val_loss_recons =  []\n",
    "    val_loss_kld =  []\n",
    "    loss_iters = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        log_epoch = (epoch % vis_frequency == 0 or epoch == num_epochs - 1)\n",
    "        loss, recons_loss, kld_loss = eval_model(\n",
    "                model=model, eval_loader=valid_loader, criterion=criterion,\n",
    "                device=device, epoch=epoch, savefig=log_epoch, savepath=savepath,\n",
    "                writer=writer\n",
    "            )\n",
    "        val_loss.append(loss)\n",
    "        val_loss_recons.append(recons_loss)\n",
    "        val_loss_kld.append(kld_loss)\n",
    "\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/All_Valid_Loss', {\"recons\": recons_loss.item(), \"kld\": kld_loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/Comb', {\"train\": mean_loss.item(), \"valid\": loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # PLATEAU SCHEDULER\n",
    "        scheduler.step(val_loss[-1])\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % save_frequency == 0):\n",
    "            stats = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"loss_iters\": loss_iters\n",
    "            }\n",
    "            save_model(model=model,model_name='ConvolutionalVAE',lambda_kld=round(val_loss_kld[-1], 5), optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        \n",
    "        if(log_epoch):\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"       Valid loss recons: {round(val_loss_recons[-1], 5)}\")\n",
    "            print(f\"       Valid loss KL-D:   {round(val_loss_kld[-1], 5)}\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model param count=590851\n",
      "model=ConvolutionalVAE(\n",
      "  (activation_hidden): ReLU()\n",
      "  (activation_final): Sigmoid()\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU()\n",
      "    (12): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU()\n",
      "    (9): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      "  (fc_mu): Linear(in_features=2048, out_features=64, bias=True)\n",
      "  (fc_sigma): Linear(in_features=2048, out_features=64, bias=True)\n",
      "  (latent_to_decoder): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=2048, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/masroora1/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalVAE(in_size=(config[\"img_channels\"],config[\"img_size\"],config[\"img_size\"]), \n",
    "                         channel_sizes=[3,16,32,64,128], \n",
    "                         act_hidden=\"ReLU\",\n",
    "                         latent_dim=64,\n",
    "                         stride=2,\n",
    "                         padding=1,\n",
    "                         kernel_size=3).to(device)\n",
    "#print(model)\n",
    "print(f\"model param count={count_model_params(model)}\")\n",
    "writer=getTensorboardWriter(params=[os.getcwd(),\"tboard_logs\", \"cvae\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "# Decay LR by a factor of 10 after 5 epochs with no improvement\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1, verbose=True)\n",
    "print(f\"{model=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Iter 56: loss 0.97364. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:21<00:00,  2.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 1.1581\n",
      "    Valid loss: 1.38967\n",
      "       Valid loss recons: 1.38967\n",
      "       Valid loss KL-D:   0.02078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Iter 56: loss 0.91454. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Iter 56: loss 0.87104. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.91643\n",
      "    Valid loss: 0.9305\n",
      "       Valid loss recons: 0.9305\n",
      "       Valid loss KL-D:   2960.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Iter 56: loss 0.84527. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Iter 56: loss 0.91920. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.86678\n",
      "    Valid loss: 0.87302\n",
      "       Valid loss recons: 0.87302\n",
      "       Valid loss KL-D:   2129.58337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Iter 56: loss 0.84283. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Iter 56: loss 0.82492. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.83317\n",
      "    Valid loss: 0.83699\n",
      "       Valid loss recons: 0.83699\n",
      "       Valid loss KL-D:   1955.84172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Iter 56: loss 0.82035. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Iter 56: loss 0.81397. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.8052\n",
      "    Valid loss: 0.80987\n",
      "       Valid loss recons: 0.80987\n",
      "       Valid loss KL-D:   1737.82827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Iter 56: loss 0.80765. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Iter 56: loss 0.72934. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.78327\n",
      "    Valid loss: 0.78601\n",
      "       Valid loss recons: 0.78601\n",
      "       Valid loss KL-D:   2066.94779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Iter 56: loss 0.79950. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Iter 56: loss 0.78579. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.76535\n",
      "    Valid loss: 0.76977\n",
      "       Valid loss recons: 0.76977\n",
      "       Valid loss KL-D:   2125.10307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Iter 56: loss 0.71030. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:19<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([256, 3, 64, 64])\n",
      "****images.shape=torch.Size([187, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Iter 56: loss 0.74139. : 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 56/56 [00:20<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Train loss: 0.72582\n",
      "    Valid loss: 0.72928\n",
      "       Valid loss recons: 0.72928\n",
      "       Valid loss KL-D:   2364.07155\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=vae_loss_function,\n",
    "        train_loader=train_loader, valid_loader=valid_loader, num_epochs=config[\"num_epochs\"], savepath=config[\"savepath\"],\n",
    "        writer=writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_image_size(config[\"img_size\"], model.kernel_size, model.padding, model.stride)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
