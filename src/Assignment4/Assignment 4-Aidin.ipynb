{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    " - a) We will be working with the **AFHQ (Animal Faces-HQ)** dataset: https://github.com/clovaai/stargan-v2\n",
    "    - Approx 15.000 images of animal faces (cat, dog, tiger, ...)\n",
    "    - Downsample the images to 64x64\n",
    " - b) Write **Convolutional** Variational Autoencoder (ConvVAE)\n",
    "      - Use Conv. layers for encoder and Transposed-Conv. layers for decoder.\n",
    "      - You are only allowed to use linear layers for estimating the mean and standard deviation. Everything else should be convolutional.\n",
    " - c) Investigate the importance of the KL-divergence weight. For this purpose, train multiple models (at least 4) using different weighting values and investigate how this value affects the generation performance.\n",
    " - d) Generate new images by sampling latent vectors, investigate latent space and visualize some interpolations.\n",
    " - e) Compare the models from b) and c)\n",
    "     - Qualitative comparison. Which images look better?\n",
    "     - Quantitative comparison between models using the Fr√©chet Inception Distance: https://arxiv.org/abs/1706.08500\n",
    "     - Log generated images and losses into the Tensorboard/W&B\n",
    "     \n",
    "     \n",
    "**Extra Point:**\n",
    " - Extend your ConvVAE for Image generation conditioned on a given class. The AFHQ dataset has 3 classes: 'cat', 'dog', and 'wildlife'\n",
    " - Train your Conditional-ConvVAE\n",
    " - Show that you can generate images conditioned on a label\n",
    " - Tutorial: https://ijdykeman.github.io/ml/2016/12/21/cvae.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 10:00:26.338230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-03 10:00:26.846540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image\n",
    "from utils import *\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"data_dir\": \"data/AFHQ\",\n",
    "        \"img_size\": 64,\n",
    "        \"img_channels\": 3,\n",
    "        \"batch_size\": 256,\n",
    "        \"num_workers\": 8,\n",
    "        \"savepath\" : \"imgs/vanilla_vae\",\n",
    "        \"num_epochs\": 15\n",
    "        }\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing mean, std of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'computed:\\nMean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\\nStd  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.ImageFolder(\n",
    "    root=config[\"data_dir\"]+\"/train\",\n",
    "    transform=transforms.ToTensor()  # Converts images to [C, H, W] with values in [0, 1]\n",
    ")\n",
    "\"\"\"mean, std = compute_stats(dataset)\n",
    "print(f\"Mean (R, G, B): {mean.tolist()}\")\n",
    "print(f\"Std  (R, G, B): {std.tolist()}\")\n",
    "\"\"\"\n",
    "\n",
    "mean = torch.tensor([0.5018709301948547, 0.4601306915283203, 0.3988320827484131])\n",
    "std = torch.tensor([0.22490206360816956, 0.2184346467256546, 0.21783076226711273])\n",
    "\"\"\"computed:\n",
    "Mean (R, G, B): [0.5018709301948547, 0.4601306915283203, 0.3988320827484131]\n",
    "Std  (R, G, B): [0.22490206360816956, 0.2184346467256546, 0.21783076226711273]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 14336 images\n",
      "Valdiation set size: 1467 images\n",
      "torch.Size([256, 3, 64, 64])\n",
      "['cat', 'dog', 'wild']\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(config[\"img_size\"]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "}\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"train\"), data_transforms[\"train\"])\n",
    "valid_dataset = datasets.ImageFolder(os.path.join(config[\"data_dir\"], \"test\"), data_transforms[\"val\"])\n",
    "\n",
    "N_train = len(train_dataset)\n",
    "N_valid = len(valid_dataset)\n",
    "print(f\"Training set size: {N_train} images\")\n",
    "print(f\"Valdiation set size: {N_valid} images\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, config[\"batch_size\"], shuffle=True, num_workers=config[\"num_workers\"])\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, config[\"batch_size\"], shuffle=False, num_workers=config[\"num_workers\"])\n",
    "\n",
    "for i, (images, _) in enumerate(valid_loader):\n",
    "    print(images.shape)\n",
    "    break\n",
    "\n",
    "class_names = train_dataset.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_grid(data, titles=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    data = data.numpy().transpose((0, 2, 3, 1))\n",
    "    data = std * data + mean  # IMPORTANT! If you normalize imgs in the DataLoader, undo the norm. for visualization\n",
    "    data = np.clip(data, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(8*2, 4*2))\n",
    "    for i in range(32):\n",
    "        plt.subplot(4,8,i+1)\n",
    "        plt.imshow(data[i])\n",
    "        plt.axis(\"off\")\n",
    "        if titles is not None:\n",
    "            plt.title(titles[i])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "            \n",
    "# Get a batch of training data and displaying it\n",
    "#inputs, classes = next(iter(train_loader))\n",
    "#inputs, classes = next(iter(valid_loader))\n",
    "#titles = [class_names[x] for x in classes]\n",
    "#inputs.shape\n",
    "#show_grid(inputs, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational Auto Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(self, in_size=(3, 32, 32), \n",
    "                 channel_sizes=[3,16,32,64,128],\n",
    "                 latent_dim=64,\n",
    "                 act_final=\"Sigmoid\",\n",
    "                 act_hidden=\"ReLU\", \n",
    "                 kernel_size=3, \n",
    "                 padding=1,\n",
    "                 stride=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.channel_sizes = channel_sizes\n",
    "        self.activation_hidden = get_activation(act_hidden)\n",
    "        self.activation_final= get_activation(act_final)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder,self.latent_input_size = self._make_encoder()\n",
    "        self.decoder = self._make_decoder()\n",
    "        self.fc_mu = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        self.fc_sigma = nn.Linear(self.latent_input_size, latent_dim)\n",
    "        return\n",
    "        \n",
    "    def _make_encoder(self):\n",
    "        \"\"\" Defining encoder \"\"\"\n",
    "        #layers = [nn.Flatten()]\n",
    "        layers = []\n",
    "        for i in range(len(self.channel_sizes)-1):\n",
    "            layers.append( nn.Conv2d(in_channels=self.channel_sizes[i], \n",
    "                                     out_channels=self.channel_sizes[i+1], \n",
    "                                     kernel_size=self.kernel_size, \n",
    "                                     padding=self.padding,\n",
    "                                     stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[i+1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "            if i ==0:\n",
    "                img_size = compute_image_size(list(self.in_size[1:3]), np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "            else:\n",
    "                img_size = compute_image_size(img_size, np.ones(2)*self.kernel_size, np.ones(2)*self.padding, np.ones(2)*self.stride)\n",
    "        \n",
    "        layers.append(nn.Flatten())\n",
    "        encoder = nn.Sequential(*layers)\n",
    "        latent_input_size=int(np.prod(img_size)*self.channel_sizes[-1])\n",
    "        return encoder, latent_input_size\n",
    "    \n",
    "    def _make_decoder(self):\n",
    "        \"\"\" Defining decoder \"\"\"\n",
    "        layers = [nn.Linear(in_features=self.latent_dim,out_features=self.latent_input_size),self.activation_hidden]\n",
    "        \n",
    "        for i in range(1, len(self.channel_sizes)):\n",
    "            layers.append( nn.ConvTranspose2d(in_channels=self.channel_sizes[-i], \n",
    "                                              out_channels=self.channel_sizes[-i-1], \n",
    "                                              kernel_size=self.kernel_size, \n",
    "                                              padding=self.padding,\n",
    "                                              stride=self.stride) )\n",
    "            layers.append(nn.BatchNorm2d(self.channel_sizes[-i-1]))\n",
    "            layers.append(self.activation_hidden)\n",
    "\n",
    "        layers = layers[:-1] + [self.activation_final]\n",
    "        decoder = nn.Sequential(*layers)\n",
    "        return decoder\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\" Reparametrization trick\"\"\"\n",
    "        std = torch.exp(0.5*log_var)  # we can also predict the std directly, but this works best\n",
    "        eps = torch.randn_like(std)  # random sampling happens here\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass \"\"\"\n",
    "        x_enc = self.encoder(x)\n",
    "        mu = self.fc_mu(x_enc)\n",
    "        log_var = self.fc_sigma(x_enc)\n",
    "        #z = self.reparameterize(mu, log_var)\n",
    "        #x_hat_flat = self.decoder(z)\n",
    "        #x_hat = x_hat_flat.view(-1, *self.in_size)\n",
    "        #return x_hat, (z, mu, log_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvae_loss_function(recons, target, mu, log_var, lambda_kld=1e-3):\n",
    "    recons_loss = F.mse_loss(recons, target)\n",
    "    kld = (-0.5 * (1 + log_var - mu**2 - log_var.exp()).sum(dim=1)).mean(dim=0)  # closed-form solution of KLD in Gaussian\n",
    "    loss = recons_loss + lambda_kld * kld\n",
    "    return loss, (recons_loss, kld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    vae_loss = []\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, _) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass\n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "         \n",
    "        # Calculate Loss\n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        vae_loss.append(kld.item())\n",
    "        \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Iter {i+1}: loss {loss.item():.5f}. \")\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    \n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device, epoch=None, savefig=False, savepath=\"\", writer=None):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    loss_list = []\n",
    "    recons_loss = []\n",
    "    kld_loss = []\n",
    "    \n",
    "    for i, (images, _) in enumerate(eval_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        # Forward pass \n",
    "        recons, (z, mu, log_var) = model(images)\n",
    "                 \n",
    "        loss, (mse, kld) = criterion(recons, images, mu, log_var)\n",
    "        loss_list.append(loss.item())\n",
    "        recons_loss.append(mse.item())\n",
    "        kld_loss.append(kld.item())\n",
    "        \n",
    "        if(i==0 and savefig):\n",
    "            save_image( recons[:64].cpu(), os.path.join(savepath, f\"recons{epoch}.png\") )\n",
    "            if writer is not None:\n",
    "                grid = torchvision.utils.make_grid(images[:64].cpu())\n",
    "                writer.add_image('images', grid, epoch)\n",
    "                grid = torchvision.utils.make_grid(recons[:64].cpu())\n",
    "                writer.add_image('output_images', grid, epoch)\n",
    "            \n",
    "    # Total correct predictions and loss\n",
    "    loss = np.mean(loss_list)\n",
    "    recons_loss = np.mean(recons_loss)\n",
    "    kld_loss = np.mean(kld_loss)\n",
    "    return loss, recons_loss, kld_loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader,\n",
    "                num_epochs, savepath, writer, save_frequency=5, vis_frequency=2):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    val_loss_recons =  []\n",
    "    val_loss_kld =  []\n",
    "    loss_iters = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        log_epoch = (epoch % vis_frequency == 0 or epoch == num_epochs - 1)\n",
    "        loss, recons_loss, kld_loss = eval_model(\n",
    "                model=model, eval_loader=valid_loader, criterion=criterion,\n",
    "                device=device, epoch=epoch, savefig=log_epoch, savepath=savepath,\n",
    "                writer=writer\n",
    "            )\n",
    "        val_loss.append(loss)\n",
    "        val_loss_recons.append(recons_loss)\n",
    "        val_loss_kld.append(kld_loss)\n",
    "\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/All_Valid_Loss', {\"recons\": recons_loss.item(), \"kld\": kld_loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch)\n",
    "        writer.add_scalars(f'Loss/Comb', {\"train\": mean_loss.item(), \"valid\": loss.item()}, global_step=epoch)\n",
    "        \n",
    "        # PLATEAU SCHEDULER\n",
    "        scheduler.step(val_loss[-1])\n",
    "        train_loss.append(mean_loss)\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % save_frequency == 0):\n",
    "            stats = {\n",
    "                \"train_loss\": train_loss,\n",
    "                \"valid_loss\": val_loss,\n",
    "                \"loss_iters\": loss_iters\n",
    "            }\n",
    "            save_model(model=model, optimizer=optimizer, epoch=epoch, stats=stats)\n",
    "        \n",
    "        if(log_epoch):\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"       Valid loss recons: {round(val_loss_recons[-1], 5)}\")\n",
    "            print(f\"       Valid loss KL-D:   {round(val_loss_kld[-1], 5)}\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model param count=590857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/masroora1/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvolutionalVAE(\n",
       "  (activation_hidden): ReLU()\n",
       "  (activation_final): Sigmoid()\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): ReLU()\n",
       "    (9): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): ReLU()\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=2048, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): ReLU()\n",
       "    (5): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): ReLU()\n",
       "    (8): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): ConvTranspose2d(16, 3, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (13): Sigmoid()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=2048, out_features=64, bias=True)\n",
       "  (fc_sigma): Linear(in_features=2048, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvolutionalVAE(in_size=(config[\"img_channels\"],config[\"img_size\"],config[\"img_size\"]), \n",
    "                         channel_sizes=[3,16,32,64,128], \n",
    "                         act_hidden=\"ReLU\",\n",
    "                         latent_dim=64,\n",
    "                         stride=2,\n",
    "                         padding=1,\n",
    "                         kernel_size=3).to(device)\n",
    "#print(model)\n",
    "print(f\"model param count={count_model_params(model)}\")\n",
    "writer=getTensorboardWriter(params=[os.getcwd(),\"tboard_logs\", \"cvae\"])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "# Decay LR by a factor of 10 after 5 epochs with no improvement\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1, verbose=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_loss_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msavepath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, savepath, writer, save_frequency, vis_frequency)\u001b[0m\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# important for dropout and batch norms\u001b[39;00m\n\u001b[1;32m     84\u001b[0m log_epoch \u001b[38;5;241m=\u001b[39m (epoch \u001b[38;5;241m%\u001b[39m vis_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m loss, recons_loss, kld_loss \u001b[38;5;241m=\u001b[39m \u001b[43meval_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavefig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msavepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msavepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m val_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     91\u001b[0m val_loss_recons\u001b[38;5;241m.\u001b[39mappend(recons_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36meval_model\u001b[0;34m(model, eval_loader, criterion, device, epoch, savefig, savepath, writer)\u001b[0m\n\u001b[1;32m     41\u001b[0m recons_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m kld_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(eval_loader):\n\u001b[1;32m     45\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Forward pass \u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss, val_loss, loss_iters, val_loss_recons, val_loss_kld = train_model(\n",
    "        model=model, optimizer=optimizer, scheduler=scheduler, criterion=vae_loss_function,\n",
    "        train_loader=train_loader, valid_loader=valid_loader, num_epochs=config[\"num_epochs\"], savepath=config[\"savepath\"],\n",
    "        writer=writer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_image_size(config[\"img_size\"], model.kernel_size, model.padding, model.stride)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
